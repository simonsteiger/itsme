[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simon Steiger",
    "section": "",
    "text": "This is my Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog landing page",
    "section": "",
    "text": "K-means homebrew üç∫\n\n\nA step-by-step walkthrough of a simple clustering algorithm\n\n\n\n\nJulia\n\n\nStatistics\n\n\nClustering\n\n\n \n\n\n\n\nNov 2, 2023\n\n\nSimon Steiger\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html",
    "href": "blog/2023-11-02-kmeans/index.html",
    "title": "K-means homebrew üç∫",
    "section": "",
    "text": "This is my attempt at coding \\(K\\)-means from scratch. I have used the pseudocode from this handout on AI systems as a starting point but so far not looked into optimized solutions. I can already think of a few improvements which would affect efficiency though, so this version is certainly clunkier and slower than necessary. üê¢\nLet‚Äôs begin by describing the goal of \\(K\\)-means mathematically.\n\\[\n\\text{minimize} \\ \\sum_{i=1}^n \\| \\mathbf{x}_i - \\boldsymbol{\\mu}_{z_i} \\|^2 \\ \\text{w.r.t.} \\ (\\boldsymbol{\\mu}, z)\n\\]\nIn this equation, \\(\\boldsymbol{\\mu}_k\\) is the center of the \\(k\\)-th cluster, and \\(z_i\\) is an index of the cluster for \\(i\\)-th point \\(\\mathbf{x}_i\\). I have also seen \\(K\\)-means being described as a coordinate-descent algorithm, but I am not yet deep enough into the terminology to understand that well."
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#the-core",
    "href": "blog/2023-11-02-kmeans/index.html#the-core",
    "title": "K-means homebrew üç∫",
    "section": "",
    "text": "I will start by laying out the general structure of the algorithm and then defining its components one by one.\n\nfunction kmeans(k, X; max_iter=1000, seed=42)\n    \n    # Dimensionality of the problem\n    nfeatures, nobs = size(X)\n\n    # Initialize random centroids and empty label vector\n    initctr = initcentroids(nfeatures, k; seed)\n    initlbl = Vector{Int64}(undef, nobs)\n\n    # Track the state of the algorithm\n    state = KmeansResult(k, X, initctr, initlbl, 0, seed, [], [])\n    \n    # Minimize Euclidean distance until convergence or maximum iterations\n    while notconverged(state.log_centroids, state.iter, max_iter)\n\n        # Assign labels to each observation based on centroid location\n        assign!(state.labels, X, state.centroids)\n        push!(state.log_labels, copy(state.labels))\n\n        # Reposition centroids based on assigned observations\n        reposition!(state.centroids, X, state.labels, k, state.iter)\n        push!(state.log_centroids, copy(state.centroids))\n\n        # Increment iteration counter\n        state.iter += 1\n    end\n\n    # Warn if no convergence after reaching iteration limit\n    state.iter == max_iter && @warn \"Did not converge after $max_iter iterations.\"\n    \n    return state\nend;"
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#the-maths",
    "href": "blog/2023-11-02-kmeans/index.html#the-maths",
    "title": "K-means homebrew üç∫",
    "section": "",
    "text": "This is my attempt at coding K-means from scratch. I have used the pseudocode from this handout on AI systems as a starting point but so far not looked into optimized solutions. I can already think of a few improvements which would affect efficiency though, so this version is certainly clunkier and slower than necessary. üê¢\nLet‚Äôs begin by describing the goal of \\(K\\)-means mathematically.\n\\[\n\\text{minimize} \\ \\sum_{i=1}^n \\| \\mathbf{x}_i - \\boldsymbol{\\mu}_{z_i} \\|^2 \\ \\text{w.r.t.} \\ (\\boldsymbol{\\mu}, z)\n\\]\nIn this equation, \\(\\boldsymbol{\\mu}_k\\) is the center of the \\(k\\)-th cluster, and \\(z_i\\) is an index of the cluster for \\(i\\)-th point \\(\\mathbf{x}_i\\). I have also seen \\(K\\)-means being described as a coordinate-descent algorithm, but I am not yet deep enough into the terminology to understand that well."
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#in-a-nutshell",
    "href": "blog/2023-11-02-kmeans/index.html#in-a-nutshell",
    "title": "K-means homebrew üç∫",
    "section": "",
    "text": "This is my attempt at coding K-means from scratch. I have used the pseudocode from this handout on AI systems as a starting point but so far not looked into optimized solutions. I can already think of a few improvements which would affect efficiency though, so this version is certainly clunkier and slower than necessary. üê¢\nLet‚Äôs begin by describing the goal of \\(K\\)-means mathematically.\n\\[\n\\text{minimize} \\ \\sum_{i=1}^n \\| \\mathbf{x}_i - \\boldsymbol{\\mu}_{z_i} \\|^2 \\ \\text{w.r.t.} \\ (\\boldsymbol{\\mu}, z)\n\\]\nIn this equation, \\(\\boldsymbol{\\mu}_k\\) is the center of the \\(k\\)-th cluster, and \\(z_i\\) is an index of the cluster for \\(i\\)-th point \\(\\mathbf{x}_i\\). I have also seen \\(K\\)-means being described as a coordinate-descent algorithm, but I am not yet deep enough into the terminology to understand that well."
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#completing-the-toolbox",
    "href": "blog/2023-11-02-kmeans/index.html#completing-the-toolbox",
    "title": "K-means homebrew üç∫",
    "section": "Completing the toolbox üõ†Ô∏è",
    "text": "Completing the toolbox üõ†Ô∏è\nNow that we‚Äôve laid out the general logic we want the \\(K\\)-means algorithm to follow, we still need to craft a few tools to make it run.\nFirst up is an object class (called a struct in Julia), which will allow us to track all relevant parameters of the algorithm in a single place. This will also be the result that is returned to the user.\nIn detail, we‚Äôll track and return the following:\n\nk, the number of clusters to be fit\nX, the matrix of the data to be clustered\ncentroids, the final centroids of the clusters\nlabels, the final labels of the observations\niter, the number of iterations until the algorithm ended\nseed, the random seed used for cluster initialization\nlog_centroids, a vector of centroid-matrices, for each iteration one matrix\nlog_labels, a vector of label-vectors, for each iteration one vector\n\n\nmutable struct KmeansResult\n    k::Int64\n    X::Matrix{Float64}\n    centroids::Matrix{Float64}\n    labels::Vector{Int64}\n    iter::Int64\n    seed::Int64\n    log_centroids::AbstractArray\n    log_labels::AbstractArray\nend\n\n\nWhy mutable struct?\nUnlike mutable structs, pure structs are immutable after creation, meaning that the values stored in each field cannot be altered. The pure version would be a poor choice in our case, since we want to update the state of the algorithm at each iteration step. Therefore, we need a mutable struct. This type is generally less memory efficient and comes with reduced performance, but we can‚Äôt avoid it here (I think)!"
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#crafting-tools",
    "href": "blog/2023-11-02-kmeans/index.html#crafting-tools",
    "title": "K-means homebrew üç∫",
    "section": "Crafting tools üõ†Ô∏è",
    "text": "Crafting tools üõ†Ô∏è\nNow that we‚Äôve laid out the general logic we want the \\(K\\)-means algorithm to follow, we still need to craft a few tools to make it run.\nFirst up is an object class (called a struct in Julia), which will allow us to track all relevant parameters of the algorithm in a single place. This will also be the result that is returned to the user.\nIn detail, we‚Äôll track and return the following:\n\nk, the number of clusters to be fit\nX, the matrix of the data to be clustered\ncentroids, the final centroids of the clusters\nlabels, the final labels of the observations\niter, the number of iterations until the algorithm ended\nseed, the random seed used for cluster initialization\nlog_centroids, a vector of centroid-matrices, for each iteration one matrix\nlog_labels, a vector of label-vectors, for each iteration one vector\n\n\nmutable struct KmeansResult\n    k::Int64\n    X::Matrix{Float64}\n    centroids::Matrix{Float64}\n    labels::Vector{Int64}\n    iter::Int64\n    seed::Int64\n    log_centroids::AbstractArray\n    log_labels::AbstractArray\nend\n\n\nWhy mutable struct?\nUnlike mutable structs, pure structs are immutable after creation, meaning that the values stored in each field cannot be altered. The pure version would be a poor choice in our case, since we want to update the state of the algorithm at each iteration step. Therefore, we need a mutable struct. This type is generally less memory efficient and comes with reduced performance, but we can‚Äôt avoid it here (I think)!\n\n\nInitializing centroids\nFirst, we need a function which initializes random centroids at the beginning of the algorithm (but it‚Äôs also very useful as a reboot when one of our centroids ends up without any observations assigned to it! ü§´).\nWe‚Äôll use the Xoshiro256++ pseudorandom number generator to manually set random seeds for initial centroids.\n\nimport Random: Xoshiro\n\ninitcentroids(d, k; seed=42) = randn(Xoshiro(seed), d, k);\n\n\n\nChecking convergence\nWe need a function which determines if the \\(K\\)-means algorithm has converged, or if it has reached the maximum number of iterations.\nDuring each iteration, this function checks for three conditions.\n\n1. First iteration\nThe algorithm cannot converge during the first iteration (itr &lt; 2) because we define convergence as the absence of change in centroids between two consecutive evaluations. Hence, we need a second iteration to evaluate convergence.\n\n\n2. Static centroids\nI‚Äôve already mentioned the second condition, which is that there is no change in centroids between two consecutive iterations. Since we want the condition to evaluate to false, we have to flip it and get ctr[itr] ‚â† ctr[itr-1].\n\n\n3. Maximum iterations\nFinally, the algorithm should stop once it has reached the maximum number of iterations (itr &lt; max).\n\nnotconverged(ctr, itr, max) = (itr &lt; 2 || ctr[itr] ‚â† ctr[itr-1]) && itr &lt; max;\n\nLooking at the boolean algebra connecting these three conditions in the function above, we see that the algorithm will stop, i.e., notconverged() will return false, when both condition 1 and 2 evaluate to false, or when condition 3 evaluates to false.\n\n\n\nCalculating distances\nThe \\(K\\)-means algorithm uses Euclidean distances between the vector of the centroid(s) and each observation. We‚Äôll use a version of the formula which generalizes to higher dimensions.\n\\[\n\\Delta(p,q)={\\sqrt {(p-q)^{2}}}\n\\]\n\neuclidean(x, y) = ‚àö(sum((x .- y) .^ 2));\n\nAs it stands, this function will work well for calculating the distance between two vectors. What we really need though are pairwise distances between two sets of vectors, one being the observations and the other one being the centroids.\nTo help with this, I‚Äôll define a helper which makes the euclidean() function work as described.\n\nfunction pairwise(f, x, y; dims=1)\n    [f(i, j) for i in eachslice(x, dims=dims), j in eachslice(y, dims=dims)]\nend;\n\n\n\nAssigning labels\nWe label each observation depending on which centroids it‚Äôs closest to in Euclidean space.\nThis function could be improved because it is inefficient1 and could be prettier 2.\n\nfunction assign!(lab, X, ctr)\n    Œî = pairwise(euclidean, X, ctr, dims=2)\n    idxmin = findmin(Œî, dims=2)[2]\n    [lab[i] = idxmin[i][2] for i in eachindex(idxmin)]\nend;\n\n\n\nRepositioning centroids\nIn each iteration, we want to reposition the centroids to the means of the observations assigned to them.\nThis concept breaks down if one of the centroids is assigned no observations at all. In this scenario, we will reinitiate that centroid randomly and move to the next iteration.\n\nimport StatsBase: mean\n\nfunction reposition!(c_new, X, labels, k, iter)\n    for i in 1:k\n        idx = labels .== i\n        if all(!, idx)\n            c_new[:, i] = initcentroids(size(c_new, 1), 1; seed=iter)\n        else\n            c_new[:, i] = [mean(r) for r in eachslice(X[:, idx], dims=1)]\n        end\n    end\nend;\n\n\nWant a different random centroid?\nWe‚Äôll have to use a different seed than the one initcentroids() uses by default! Otherwise, we would initiate a new centroid in exactly the same position as the one that failed us before. I have chosen to set the seed to iter, so even if the reinitiation fails repeatedly, the algorithm will try with a new centroid each time. Finally, if it never succeeds, the algorithm still stops at upon reaching the maximum number of iterations.\n\n\n\nStandardising features\nFeatures passed to \\(K\\)-means must be standardised.\n\nstandardise(x) = (x .- mean(x)) ./ std(x);\n\nSo far so good! I think that‚Äôs all we need."
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#foods-ready",
    "href": "blog/2023-11-02-kmeans/index.html#foods-ready",
    "title": "K-means homebrew üç∫",
    "section": "Food‚Äôs ready!",
    "text": "Food‚Äôs ready!\nPlay around with the sliders to set the sample size, number of clusters, and cluster separation. Then we‚Äôll use this data to test our new \\(K\\)-means algorithm!\n\nusing StatsPlots\n\nX = reduce(hcat, generate_multivariate_data(3000, 4, 10))\n\nhistogram2d(X[1, :], X[2, :], bins=50)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\nWhat does the input to separation do?\nIf you pay attention to the axes, you‚Äôll realize that this parameter simply stretches out the space in which the gaussians are located, thus pulling them apart or moving them closer together."
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#and-action",
    "href": "blog/2023-11-02-kmeans/index.html#and-action",
    "title": "K-means homebrew üç∫",
    "section": "And action!",
    "text": "And action!\nWe will fit five clusters to this data and set a seed.\n\nk = 6;\nseed = 42;\n\nTime to fit!\n\nfit = kmeans(k, Z, seed=seed);\n\nWe can inspect the result by looking at the different fields of fit. To check the final centroids, we would do the following:\n\nfit.centroids\n\n2√ó6 Matrix{Float64}:\n -0.60709  -1.33431    1.27957    -0.601472   0.403219  0.495378\n  1.18897  -0.0280225  0.0040526  -1.21767   -1.17503   1.19287"
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#tracking-k-means",
    "href": "blog/2023-11-02-kmeans/index.html#tracking-k-means",
    "title": "K-means homebrew üç∫",
    "section": "Tracking K-means",
    "text": "Tracking K-means\nLet‚Äôs look at the steps the \\(K\\)-means algorithm takes to find its solution.\n\n@gif for i in eachindex(fit.log_centroids)\n    scatter(Z[1, :], Z[2, :], c=fit.log_labels[i], markershape=:xcross, legend=:none)\n    scatter!(fit.log_centroids[i][1, :], fit.log_centroids[i][2, :], c=[1:k;])\n    title!(\"Iteration $i\")\n    xlims!(-3, 3), ylims!(-3, 3)\nend every 1 fps=15\n\n[ Info: Saved animation to /Users/simonsteiger/Documents/GitHub/itsme/blog/2023-11-02-kmeans/tmp.gif"
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#footnotes",
    "href": "blog/2023-11-02-kmeans/index.html#footnotes",
    "title": "K-means homebrew üç∫",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nüê¢ This is currently updating the labels indiscriminately without checking if the centroid has changed at all.‚Ü©Ô∏é\nü™≥ I don‚Äôt like the way I‚Äôm accessing the CartesianIndices returned by findmin(). There is probably a better solution with view().‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#the-toolbox",
    "href": "blog/2023-11-02-kmeans/index.html#the-toolbox",
    "title": "K-means homebrew üç∫",
    "section": "",
    "text": "Now that we‚Äôve laid out the general logic we want the \\(K\\)-means algorithm to follow, we still need to craft a few tools to make it run.\nFirst up is an object class (called a struct in Julia), which will allow us to track all relevant parameters of the algorithm in a single place. This will also be the result that is returned to the user.\nIn detail, we‚Äôll track and return the following:\n\nk, the number of clusters to be fit\nX, the matrix of the data to be clustered\ncentroids, the final centroids of the clusters\nlabels, the final labels of the observations\niter, the number of iterations until the algorithm ended\nseed, the random seed used for cluster initialization\nlog_centroids, a vector of centroid-matrices, for each iteration one matrix\nlog_labels, a vector of label-vectors, for each iteration one vector\n\n\nmutable struct KmeansResult\n    k::Int64\n    X::Matrix{Float64}\n    centroids::Matrix{Float64}\n    labels::Vector{Int64}\n    iter::Int64\n    seed::Int64\n    log_centroids::AbstractArray\n    log_labels::AbstractArray\nend\n\n\n\nUnlike mutable structs, pure structs are immutable after creation, meaning that the values stored in each field cannot be altered. The pure version would be a poor choice in our case, since we want to update the state of the algorithm at each iteration step. Therefore, we need a mutable struct. This type is generally less memory efficient and comes with reduced performance, but we can‚Äôt avoid it here (I think)!\n\n\n\nFirst, we need a function which initializes random centroids at the beginning of the algorithm (but it‚Äôs also very useful as a reboot when one of our centroids ends up without any observations assigned to it! ü§´).\nWe‚Äôll use the Xoshiro256++ pseudorandom number generator to manually set random seeds for initial centroids.\n\nimport Random: Xoshiro\n\ninitcentroids(d, k; seed=42) = randn(Xoshiro(seed), d, k);\n\n\n\n\nWe need a function which determines if the \\(K\\)-means algorithm has converged, or if it has reached the maximum number of iterations.\nDuring each iteration, this function checks for three conditions.\n\n\nThe algorithm cannot converge during the first iteration (itr &lt; 2) because we define convergence as the absence of change in centroids between two consecutive evaluations. Hence, we need a second iteration to evaluate convergence.\n\n\n\nI‚Äôve already mentioned the second condition, which is that there is no change in centroids between two consecutive iterations. Since we want the condition to evaluate to false, we have to flip it and get ctr[itr] ‚â† ctr[itr-1].\n\n\n\nFinally, the algorithm should stop once it has reached the maximum number of iterations (itr &lt; max).\n\nnotconverged(ctr, itr, max) = (itr &lt; 2 || ctr[itr] ‚â† ctr[itr-1]) && itr &lt; max;\n\nLooking at the boolean algebra connecting these three conditions in the function above, we see that the algorithm will stop, i.e., notconverged() will return false, when both condition 1 and 2 evaluate to false, or when condition 3 evaluates to false.\n\n\n\n\nThe \\(K\\)-means algorithm uses Euclidean distances between the vector of the centroid(s) and each observation. We‚Äôll use a version of the formula which generalizes to higher dimensions.\n\\[\n\\Delta(p,q)={\\sqrt {(p-q)^{2}}}\n\\]\n\neuclidean(x, y) = ‚àö(sum((x .- y) .^ 2));\n\nAs it stands, this function will work well for calculating the distance between two vectors. What we really need though are pairwise distances between two sets of vectors, one being the observations and the other one being the centroids.\nTo help with this, I‚Äôll define a helper which makes the euclidean() function work as described.\n\nfunction pairwise(f, x, y; dims=1)\n    [f(i, j) for i in eachslice(x, dims=dims), j in eachslice(y, dims=dims)]\nend;\n\n\n\n\nWe label each observation depending on which centroids it‚Äôs closest to in Euclidean space.\nThis function could be improved because it is inefficient1 and could be prettier 2.\n\nfunction assign!(lab, X, ctr)\n    Œî = pairwise(euclidean, X, ctr, dims=2)\n    idxmin = findmin(Œî, dims=2)[2]\n    [lab[i] = idxmin[i][2] for i in eachindex(idxmin)]\nend;\n\n\n\n\nIn each iteration, we want to reposition the centroids to the means of the observations assigned to them.\nThis concept breaks down if one of the centroids is assigned no observations at all. In this scenario, we will reinitiate that centroid randomly and move to the next iteration.\n\nimport StatsBase: mean\n\nfunction reposition!(c_new, X, labels, k, iter)\n    for i in 1:k\n        idx = labels .== i\n        if all(!, idx)\n            c_new[:, i] = initcentroids(size(c_new, 1), 1; seed=iter)\n        else\n            c_new[:, i] = [mean(r) for r in eachslice(X[:, idx], dims=1)]\n        end\n    end\nend;\n\n\n\nWe‚Äôll have to use a different seed than the one initcentroids() uses by default! Otherwise, we would initiate a new centroid in exactly the same position as the one that failed us before. I have chosen to set the seed to iter, so even if the reinitiation fails repeatedly, the algorithm will try with a new centroid each time. Finally, if it never succeeds, the algorithm still stops at upon reaching the maximum number of iterations.\n\n\n\n\nFeatures passed to \\(K\\)-means must be standardised.\n\nstandardise(x) = (x .- mean(x)) ./ std(x);\n\nSo far so good! I think that‚Äôs all we need."
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#inspecting-the-data",
    "href": "blog/2023-11-02-kmeans/index.html#inspecting-the-data",
    "title": "K-means homebrew üç∫",
    "section": "Inspecting the data",
    "text": "Inspecting the data\nPlay around with the sliders to set the sample size, number of clusters, and cluster separation. Then we‚Äôll use this data to test our new \\(K\\)-means algorithm!\n\nusing StatsPlots\n\nX = reduce(hcat, generate_multivariate_data(3000, 4, 10))\n\nhistogram2d(X[1, :], X[2, :], bins=50)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\nWhat does the input to separation do?\nIf you pay attention to the axes, you‚Äôll realize that this parameter simply stretches out the space in which the gaussians are located, thus pulling them apart or moving them closer together."
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#main-function",
    "href": "blog/2023-11-02-kmeans/index.html#main-function",
    "title": "K-means homebrew üç∫",
    "section": "Main function",
    "text": "Main function\n\nfunction kmeans(k, X; max_iter=1000, seed=42)\n    \n    # Dimensionality of the problem\n    nfeatures, nobs = size(X)\n\n    # Initialize random centroids and empty label vector\n    initctr = initcentroids(nfeatures, k; seed)\n    initlbl = Vector{Int64}(undef, nobs)\n\n    # Track the state of the algorithm\n    state = KmeansResult(k, X, initctr, initlbl, 0, seed, [], [])\n    \n    # Minimize Euclidean distance until convergence or maximum iterations\n    while notconverged(state.log_centroids, state.iter, max_iter)\n\n        # Assign labels to each observation based on centroid location\n        assign!(state.labels, X, state.centroids)\n        push!(state.log_labels, copy(state.labels))\n\n        # Reposition centroids based on assigned observations\n        reposition!(state.centroids, X, state.labels, k, state.iter)\n        push!(state.log_centroids, copy(state.centroids))\n\n        # Increment iteration counter\n        state.iter += 1\n    end\n\n    # Warn if no convergence after reaching iteration limit\n    state.iter == max_iter && @warn \"Did not converge after $max_iter iterations.\"\n    \n    return state\nend;"
  },
  {
    "objectID": "blog/2023-11-02-kmeans/index.html#helper-functions",
    "href": "blog/2023-11-02-kmeans/index.html#helper-functions",
    "title": "K-means homebrew üç∫",
    "section": "Helper functions",
    "text": "Helper functions\nNow that we‚Äôve laid out the general logic we want the \\(K\\)-means algorithm to follow, we still need to craft a few tools to make it run.\nFirst up is an object class (called a struct in Julia), which will allow us to track all relevant parameters of the algorithm in a single place. This will also be the result that is returned to the user.\nIn detail, we‚Äôll track and return the following:\n\nk, the number of clusters to be fit\nX, the matrix of the data to be clustered\ncentroids, the final centroids of the clusters\nlabels, the final labels of the observations\niter, the number of iterations until the algorithm ended\nseed, the random seed used for cluster initialization\nlog_centroids, a vector of centroid-matrices, for each iteration one matrix\nlog_labels, a vector of label-vectors, for each iteration one vector\n\n\nmutable struct KmeansResult\n    k::Int64\n    X::Matrix{Float64}\n    centroids::Matrix{Float64}\n    labels::Vector{Int64}\n    iter::Int64\n    seed::Int64\n    log_centroids::AbstractArray\n    log_labels::AbstractArray\nend\n\n\nWhy mutable struct?\nUnlike mutable structs, pure structs are immutable after creation, meaning that the values stored in each field cannot be altered. The pure version would be a poor choice in our case, since we want to update the state of the algorithm at each iteration step. Therefore, we need a mutable struct. This type is generally less memory efficient and comes with reduced performance, but we can‚Äôt avoid it here (I think)!\n\n\nInitializing centroids\nFirst, we need a function which initializes random centroids at the beginning of the algorithm (but it‚Äôs also very useful as a reboot when one of our centroids ends up without any observations assigned to it! ü§´).\nWe‚Äôll use the Xoshiro256++ pseudorandom number generator to manually set random seeds for initial centroids.\n\nimport Random: Xoshiro\n\ninitcentroids(d, k; seed=42) = randn(Xoshiro(seed), d, k);\n\n\n\nChecking convergence\nWe need a function which determines if the \\(K\\)-means algorithm has converged, or if it has reached the maximum number of iterations.\nDuring each iteration, this function checks for three conditions.\n\n1. First iteration\nThe algorithm cannot converge during the first iteration (itr &lt; 2) because we define convergence as the absence of change in centroids between two consecutive evaluations. Hence, we need a second iteration to evaluate convergence.\n\n\n2. Static centroids\nI‚Äôve already mentioned the second condition, which is that there is no change in centroids between two consecutive iterations. Since we want the condition to evaluate to false, we have to flip it and get ctr[itr] ‚â† ctr[itr-1].\n\n\n3. Maximum iterations\nFinally, the algorithm should stop once it has reached the maximum number of iterations (itr &lt; max).\n\nnotconverged(ctr, itr, max) = (itr &lt; 2 || ctr[itr] ‚â† ctr[itr-1]) && itr &lt; max;\n\nLooking at the boolean algebra connecting these three conditions in the function above, we see that the algorithm will stop, i.e., notconverged() will return false, when both condition 1 and 2 evaluate to false, or when condition 3 evaluates to false.\n\n\n\nCalculating distances\nThe \\(K\\)-means algorithm uses Euclidean distances between the vector of the centroid(s) and each observation. We‚Äôll use a version of the formula which generalizes to higher dimensions.\n\\[\n\\Delta(p,q)={\\sqrt {(p-q)^{2}}}\n\\]\n\neuclidean(x, y) = ‚àö(sum((x .- y) .^ 2));\n\nAs it stands, this function will work well for calculating the distance between two vectors. What we really need though are pairwise distances between two sets of vectors, one being the observations and the other one being the centroids.\nTo help with this, I‚Äôll define a helper which makes the euclidean() function work as described.\n\nfunction pairwise(f, x, y; dims=1)\n    [f(i, j) for i in eachslice(x, dims=dims), j in eachslice(y, dims=dims)]\nend;\n\n\n\nAssigning labels\nWe label each observation depending on which centroids it‚Äôs closest to in Euclidean space.\nThis function could be improved because it is inefficient1 and could be prettier 2.\n\nfunction assign!(lab, X, ctr)\n    Œî = pairwise(euclidean, X, ctr, dims=2)\n    idxmin = findmin(Œî, dims=2)[2]\n    [lab[i] = idxmin[i][2] for i in eachindex(idxmin)]\nend;\n\n\n\nRepositioning centroids\nIn each iteration, we want to reposition the centroids to the means of the observations assigned to them.\nThis concept breaks down if one of the centroids is assigned no observations at all. In this scenario, we will reinitiate that centroid randomly and move to the next iteration.\n\nimport StatsBase: mean\n\nfunction reposition!(c_new, X, labels, k, iter)\n    for i in 1:k\n        idx = labels .== i\n        if all(!, idx)\n            c_new[:, i] = initcentroids(size(c_new, 1), 1; seed=iter)\n        else\n            c_new[:, i] = [mean(r) for r in eachslice(X[:, idx], dims=1)]\n        end\n    end\nend;\n\n\nWant a different random centroid?\nWe‚Äôll have to use a different seed than the one initcentroids() uses by default! Otherwise, we would initiate a new centroid in exactly the same position as the one that failed us before. I have chosen to set the seed to iter, so even if the reinitiation fails repeatedly, the algorithm will try with a new centroid each time. Finally, if it never succeeds, the algorithm still stops at upon reaching the maximum number of iterations.\n\n\n\nStandardising features\nFeatures passed to \\(K\\)-means must be standardised.\n\nstandardise(x) = (x .- mean(x)) ./ std(x);\n\nSo far so good! I think that‚Äôs all we need."
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "Simon Steiger",
    "section": "",
    "text": "K-means homebrew üç∫\n\n\n\nJulia\n\n\nStatistics\n\n\nClustering\n\n\n\n\nNov 2, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]